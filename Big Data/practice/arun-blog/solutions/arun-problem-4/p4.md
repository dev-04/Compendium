# Problem 5

## Sqoop Import as text file

```bash
sqoop import \
    --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
    --username root \
    --password cloudera \
    --table orders \
    --as-textfile \
    --target-dir /user/cloudera/problem5/text \
    --fields-terminated-by '\t' \
    --lines-terminated-by '\n'
```

## Sqoop import as avro file

```bash
sqoop import \
    --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
    --username root \
    --password cloudera \
    --table orders \
    --target-dir /user/cloudera/problem5/avro \
    --as-avrodatafile
```

## Sqoop import as parquet file

```bash
sqoop import \
    --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
    --username root \
    --password cloudera \
    --table orders \
    --target-dir /user/cloudera/problem5/parquet \
    --as-parquetfile
```

## Convert Avro data to

```scala
import com.databricks.spark.avro._
val df = sqlContext.read.avro("/user/cloudera/problem5/avro")
```

### Parquet in Snappy compression

```scala
sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")
df.write.parquet("/user/cloudera/problem5/parquet-snappy-compress")
```

### Text using Gzip Compression

```scala
val rdd = df.map(x => x(0) + "," + x(1) + "," + x(2) + "," + x(3))
rdd.saveAsTextFile("/user/cloudera/problem5/text-gzip-compress", classOf[org.apache.hadoop.io.compress.GzipCodec])
```

### Sequence File (no compression)

```scala
val rdd = df.map(x => (1, (x(0) + "," + x(1) + "," + x(2) + "," + x(3))))
rdd.saveAsSequenceFile("/user/cloudera/problem5/sequence")
```

### Text using Snappy Compression

```scala
val rdd = df.map(x=> x(0) + "," + x(1) + "," + x(2) + "," + x(3))
rdd.saveAsTextFile("/user/cloudera/problem5/text-snappy-compress", classOf[org.apache.hadoop.io.compress.SnappyCodec])
```

## Convert Parquet data to

```scala
val df = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
```

### Parquet (no compression)

```scala
sqlContext.setConf("spark.sql.parquet.compression.codec", "uncompressed")
df.write.parquet("/user/cloudera/problem5/parquet-no-compress")
```

### Avro in Snappy Compression

```scala
import com.databricks.spark.avro._
sqlContext.setConf("spark.sql.avro.compression.codec", "snappy")
df.write.avro("/user/cloudera/problem5/avro-snappy")
```

## Convert Avro (Snappy Compressed) to

```scala
import com.databricks.spark.avro._
val df = sqlContext.read.avro("/user/cloudera/problem5/avro-snappy")
```

### JSON File (no compression)

```scala
df.write.json("/user/cloudera/problem5/json-no-compress")
```

### JSON File (Gzip Compression)

```scala
df.toJSON.saveAsTextFile("/user/cloudera/problem5/json-gzip", classOf[org.apache.hadoop.io.compress.GzipCodec])
```

## JSON (GZip Compressed) to

```scala
val df = sqlContext.read.json("/user/cloudera/problem5/json-gzip")
```

### CSV File (Gzip Compressed)

```scala
val rdd = df.map(x => x(0) + "," + x(1) + "," + x(2) + "," + x(3))
rdd.saveAsTextFile("/user/cloudera/problem5/csv-gzip", classOf[org.apache.hadoop.io.compress.GzipCodec])
```

## Sequence File to ORC

```scala
val rdd = sc.sequenceFile("/user/cloudera/problem5/sequence", classOf[org.apache.hadoop.io.Text], classOf[org.apache.hadoop.io.Text])
val ord_df = rdd.map(x => {var d = x._2.toString.split(','); (d(0), d(1), d(2), d(3))}).toDF
orc_df.write.orc("/user/cloudera/problem5/orc")
```