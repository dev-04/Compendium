# P2

## Sqoop Import to text file

```bash
sqoop import \
    --connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
    --username root \
    --password cloudera \
    --table products \
    --target-dir /user/cloudera/products \
    --fields-terminated-by "|"
```

## Move HDFS directory

```bash
hdfs dfs -mkdir -p /user/cloudera/problem2
hdfs dfs -mv /user/cloudera/products /user/cloudera/problem2/
```

## Change HDFS permissions

```bash
hdfs dfs -chmod -R u=rwx,g=rw,o=rx /user/cloudera/problem2/products
```

## Spark Computation

| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |

### a - Using Dataframe

```scala
val rdd = sc.textFile("/user/cloudera/problem2/products")
val df = rdd.map(x => {var d = x.toString.split('|'); (d(0).toInt,d(1).toInt, d(2), d(3), d(4).toFloat, d(5))})

case class Product(product_id: Int, product_category_id: Int, product_name: String, product_description: String, product_price: Float, product_image: String)

val product_df = df.map(x => Product(x._1, x._2, x._3, x._4, x._5, x._6)).toDF()
val filtered_df = product_df.filter(product_df("product_price") < 100)

val final_df = filtered_df.groupBy(filtered_df("product_category_id"))
    .agg(max(col("product_price")).alias("max_price"), countDistinct(col("product_id")).alias("distinct_products"), avg(col("product_price")).alias("avg_product_price"), min(col("product_price")).alias("min_product_price"))
    .orderBy(col("product_category_id"))

import com.databricks.spark.avro._
sqlContext.setConf("spark.sql.avro.compression.codec", "snappy")
final_df.write.avro("/user/cloudera/problem2/result-df")
```

### b - Using SQL

```scala
val rdd = sc.textFile("/user/cloudera/problem2/products")
val df = rdd.map(x => {var d = x.toString.split('|'); (d(0).toInt,d(1).toInt, d(2), d(3), d(4).toFloat, d(5))})

case class Product(product_id: Int, product_category_id: Int, product_name: String, product_description: String, product_price: Float, product_image: String)

val product_df = df.map(x => Product(x._1, x._2, x._3, x._4, x._5, x._6)).toDF()

product_df.registerTempTable("products")

val final_df = sqlContext.sql("""
    SELECT product_category_id, max(product_price) as max_price, count(distinct(product_id)) as distinct_products, avg(product_price) as avg_product_prices, min(product_price) as min_product_price
    FROM products
    WHERE product_price < 100
    GROUP BY product_category_id
    ORDER BY product_category_id
""")

import com.databricks.spark.avro._
final_df.write.avro("/user/cloudera/problem2/result-sql")

```

### c - Using RDD

```scala
val rdd = sc.textFile("/user/cloudera/problem2/products")
val df = rdd.map(x => {var d = x.toString.split('|'); (d(0).toInt,d(1).toInt, d(2), d(3), d(4).toFloat, d(5))})

// (cat_id, (prod_id, price))
val rdd = df.filter(x => x._5 < 100 ).map(x => (x._2, (x._1, x._5)))

// min, max, sum, count, Set
val final_rdd = rdd.aggregateByKey(((0.0).toFloat, (0.0).toFloat, (0.0).toFloat, 0, Set[Int](0)))(
    //seqOp (U,V => U)
    (c, v) => (Math.min(c._1, v._2), Math.max(c._2, v._2), c._3+v._2, c._4+1, c._5 + v._1),
    //combOp (U, U) => U
    (c1, c2) => (Math.min(c1._1, c2._1), Math.max(c1._2, c2._2), c1._3+c2._3, c1._4+c2._4, c1._5 ++ c2._5 )
)

val final_df = final_rdd.map{ case (k, x) => (k, x._1, x._2, x._3/x._4, x._5.size) }.sortBy(_._1).toDF

import com.databricks.spark.avro._
final_df.write.avro("/user/cloudera/problem2/result-rdd")

```

## Lessons

1. Use `.split('|')` or `.split("\\|")` not `.split("|")`. `|` is a metacharacter in regex and thus escaping is required.
1. Writing avro can be done after creating dataframe.
1. Final row should be laid out in tuple like fashion.(in most cases, unless specified otherwise)
1. Always eliminate extra columns which are not important for the aggregation.
